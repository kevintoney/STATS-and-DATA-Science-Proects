{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm Detection Using XGBoost and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "#some eda from https://www.kaggle.com/danofer/loading-sarcasm-data\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from subprocess import check_output\n",
    "from matplotlib import pyplot as plot\n",
    "import os\n",
    "import numpy as np\n",
    "import xgboost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import preprocessing, metrics, svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data Exploration\n",
    "\n",
    "Repeating a portion of the data exploration from YURY KASHNITSKY: https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1010826, 10)\n",
      "Index(['label', 'comment', 'author', 'subreddit', 'score', 'ups', 'downs',\n",
      "       'date', 'created_utc', 'parent_comment'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Oh sure, the *black* duck has to be the angry one...',\n",
       " \"I'm sure they'll outlive the new Domino's that's going in.\",\n",
       " \"I bet it's Seager.\",\n",
       " 'How dare you post a shot of your computer while actually using it!',\n",
       " 'Because its not like the breaks are there for stopping or anything.',\n",
       " 'Screw this computer, it doesnt even have a Touchbar',\n",
       " 'Yes however we retain a modicum of culture',\n",
       " 'Nah, google + integration is better.',\n",
       " \"because they haven't been competitive all year long\",\n",
       " 'But if they do, rents will become unaffordable... oh wait, right, never mind.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAE8BJREFUeJzt3H9sXeV9x/H3t6S0WVoKJcWKErYwNZWagtpSi2aqtLmlooZOhD9gCqIjoGiRGJ26FW1Ntz/YYEiwiTGBKFs2ooSKFrJuXaISmkXAVbcJKGG0hB9DcWkGHlEzCGS4qHRpv/vjPka35tr3iWPfE+P3S7ryOd/znPM8j+P44/Pj3shMJEmq8bamByBJmjsMDUlSNUNDklTN0JAkVTM0JEnVDA1JUjVDQ5JUzdCQJFUzNCRJ1RY0PYCZtnjx4ly+fPm09v3xj3/MokWLZnZAxzjnPD8457e+o53vo48++mJmvq9Xu7dcaCxfvpzdu3dPa99Wq8XQ0NDMDugY55znB+f81ne0842I/6pp5+UpSVI1Q0OSVM3QkCRVMzQkSdUMDUlSNUNDklTN0JAkVasKjYjYFxF7IuJ7EbG71N4bEbsiYm/5elKpR0TcHBEjEfF4RJzZcZy1pf3eiFjbUf9YOf5I2Tem6kOS1IwjOdP4ZGZ+JDMHy/oG4L7MXAHcV9YBzgVWlNd64DZoBwBwNfBx4Czg6o4QuK20Hd9vuEcfkqQGHM07wlcDQ2V5C9ACvlTqd2RmAg9FxIkRsaS03ZWZBwEiYhcwHBEt4ITMfLDU7wAuAO6doo9Zsee/D3HZhntm6/CT2nf9Z/vep6TZsbyB3yEAm4f785EptWcaCfxLRDwaEetLbSAz9wOUr6eU+lLg+Y59R0ttqvpol/pUfUiSGlB7pvGJzHwhIk4BdkXEf07RNrrUchr1aiXI1gMMDAzQarWOZPc3DCyEq844PK19j8Z0xzsTxsbGGu2/Cc55fmhqzk38DoH+zbcqNDLzhfL1QER8k/Y9iR9FxJLM3F8uPx0ozUeBUzt2Xwa8UOpDE+qtUl/WpT1T9DFxfBuBjQCDg4M53Q/tuuXObdy4p/+f4bjvkqG+9zluvn2oGzjn+aKpOTdxiRval6f6Md+el6ciYlFEvHt8GTgHeALYDow/AbUW2FaWtwOXlqeoVgGHyqWlncA5EXFSuQF+DrCzbHs1IlaVp6YunXCsbn1IkhpQ82f1APDN8hTsAuBrmfntiHgE2BoR64DngItK+x3AecAI8BpwOUBmHoyIa4FHSrtrxm+KA1cAm4GFtG+A31vq10/ShySpAT1DIzOfBT7cpf4ScHaXegJXTnKsTcCmLvXdwOm1fUiSmuE7wiVJ1QwNSVI1Q0OSVM3QkCRVMzQkSdUMDUlSNUNDklTN0JAkVTM0JEnVDA1JUjVDQ5JUzdCQJFUzNCRJ1QwNSVI1Q0OSVM3QkCRVMzQkSdUMDUlSNUNDklTN0JAkVTM0JEnVDA1JUjVDQ5JUzdCQJFUzNCRJ1QwNSVI1Q0OSVM3QkCRVMzQkSdUMDUlSNUNDklTN0JAkVasOjYg4LiIei4hvlfXTIuLhiNgbEXdHxPGl/o6yPlK2L+84xpdL/ZmI+ExHfbjURiJiQ0e9ax+SpGYcyZnGF4CnO9ZvAG7KzBXAy8C6Ul8HvJyZ7wduKu2IiJXAGuBDwDDwlRJExwG3AucCK4GLS9up+pAkNaAqNCJiGfBZ4O/LegCfAr5RmmwBLijLq8s6ZfvZpf1q4K7MfD0zfwiMAGeV10hmPpuZPwXuAlb36EOS1IDaM42/Bv4I+HlZPxl4JTMPl/VRYGlZXgo8D1C2Hyrt36hP2Gey+lR9SJIasKBXg4j4TeBAZj4aEUPj5S5Ns8e2yerdgmuq9t3GuB5YDzAwMECr1erWrKeBhXDVGYd7N5xh0x3vTBgbG2u0/yY45/mhqTk38TsE+jffnqEBfAI4PyLOA94JnED7zOPEiFhQzgSWAS+U9qPAqcBoRCwA3gMc7KiP69ynW/3FKfr4BZm5EdgIMDg4mENDQxXTerNb7tzGjXtqviUza98lQ33vc1yr1WK636+5yjnPD03N+bIN9/S9T4DNw4v6Mt+el6cy88uZuSwzl9O+kX1/Zl4CPABcWJqtBbaV5e1lnbL9/szMUl9Tnq46DVgBfBd4BFhRnpQ6vvSxvewzWR+SpAYczfs0vgR8MSJGaN9/uL3UbwdOLvUvAhsAMvNJYCvwFPBt4MrM/Fk5i/g8sJP201lbS9up+pAkNeCIrsVkZgtoleVnaT/5NLHNT4CLJtn/OuC6LvUdwI4u9a59SJKa4TvCJUnVDA1JUjVDQ5JUzdCQJFUzNCRJ1QwNSVI1Q0OSVM3QkCRVMzQkSdUMDUlSNUNDklTN0JAkVTM0JEnVDA1JUjVDQ5JUzdCQJFUzNCRJ1QwNSVI1Q0OSVM3QkCRVMzQkSdUMDUlSNUNDklTN0JAkVTM0JEnVDA1JUjVDQ5JUzdCQJFUzNCRJ1QwNSVI1Q0OSVM3QkCRV6xkaEfHOiPhuRHw/Ip6MiD8r9dMi4uGI2BsRd0fE8aX+jrI+UrYv7zjWl0v9mYj4TEd9uNRGImJDR71rH5KkZtScabwOfCozPwx8BBiOiFXADcBNmbkCeBlYV9qvA17OzPcDN5V2RMRKYA3wIWAY+EpEHBcRxwG3AucCK4GLS1um6EOS1ICeoZFtY2X17eWVwKeAb5T6FuCCsry6rFO2nx0RUep3ZebrmflDYAQ4q7xGMvPZzPwpcBewuuwzWR+SpAZU3dMoZwTfAw4Au4AfAK9k5uHSZBRYWpaXAs8DlO2HgJM76xP2max+8hR9SJIasKCmUWb+DPhIRJwIfBP4YLdm5WtMsm2yerfgmqr9m0TEemA9wMDAAK1Wq1uzngYWwlVnHO7dcIZNd7wzYWxsrNH+m+Cc54em5tzE7xDo33yrQmNcZr4SES1gFXBiRCwoZwLLgBdKs1HgVGA0IhYA7wEOdtTHde7Trf7iFH1MHNdGYCPA4OBgDg0NHcm03nDLndu4cc8RfUtmxL5Lhvre57hWq8V0v19zlXOeH5qa82Ub7ul7nwCbhxf1Zb41T0+9r5xhEBELgU8DTwMPABeWZmuBbWV5e1mnbL8/M7PU15Snq04DVgDfBR4BVpQnpY6nfbN8e9lnsj4kSQ2o+bN6CbClPOX0NmBrZn4rIp4C7oqIPwceA24v7W8HvhoRI7TPMNYAZOaTEbEVeAo4DFxZLnsREZ8HdgLHAZsy88lyrC9N0ockqQE9QyMzHwc+2qX+LO0nnybWfwJcNMmxrgOu61LfAeyo7UOS1AzfES5JqmZoSJKqGRqSpGqGhiSpmqEhSapmaEiSqhkakqRqhoYkqZqhIUmqZmhIkqoZGpKkaoaGJKmaoSFJqmZoSJKqGRqSpGqGhiSpmqEhSapmaEiSqhkakqRqhoYkqZqhIUmqZmhIkqoZGpKkaoaGJKmaoSFJqmZoSJKqGRqSpGqGhiSpmqEhSapmaEiSqhkakqRqhoYkqVrP0IiIUyPigYh4OiKejIgvlPp7I2JXROwtX08q9YiImyNiJCIej4gzO461trTfGxFrO+ofi4g9ZZ+bIyKm6kOS1IyaM43DwFWZ+UFgFXBlRKwENgD3ZeYK4L6yDnAusKK81gO3QTsAgKuBjwNnAVd3hMBtpe34fsOlPlkfkqQG9AyNzNyfmf9Rll8FngaWAquBLaXZFuCCsrwauCPbHgJOjIglwGeAXZl5MDNfBnYBw2XbCZn5YGYmcMeEY3XrQ5LUgCO6pxERy4GPAg8DA5m5H9rBApxSmi0Fnu/YbbTUpqqPdqkzRR+SpAYsqG0YEe8C/hH4/cz833LboWvTLrWcRr1aRKynfXmLgYEBWq3Wkez+hoGFcNUZh6e179GY7nhnwtjYWKP9N8E5zw9NzbmJ3yHQv/lWhUZEvJ12YNyZmf9Uyj+KiCWZub9cYjpQ6qPAqR27LwNeKPWhCfVWqS/r0n6qPn5BZm4ENgIMDg7m0NBQt2Y93XLnNm7cU52jM2bfJUN973Ncq9Viut+vuco5zw9NzfmyDff0vU+AzcOL+jLfmqenArgdeDoz/6pj03Zg/AmotcC2jvql5SmqVcChcmlpJ3BORJxUboCfA+ws216NiFWlr0snHKtbH5KkBtT8Wf0J4LeBPRHxvVL7Y+B6YGtErAOeAy4q23YA5wEjwGvA5QCZeTAirgUeKe2uycyDZfkKYDOwELi3vJiiD0lSA3qGRmb+G93vOwCc3aV9AldOcqxNwKYu9d3A6V3qL3XrQ5LUDN8RLkmqZmhIkqoZGpKkaoaGJKmaoSFJqmZoSJKqGRqSpGqGhiSpmqEhSapmaEiSqhkakqRqhoYkqZqhIUmqZmhIkqoZGpKkaoaGJKmaoSFJqmZoSJKqGRqSpGqGhiSpmqEhSapmaEiSqhkakqRqhoYkqZqhIUmqZmhIkqoZGpKkaoaGJKmaoSFJqmZoSJKqGRqSpGqGhiSpWs/QiIhNEXEgIp7oqL03InZFxN7y9aRSj4i4OSJGIuLxiDizY5+1pf3eiFjbUf9YROwp+9wcETFVH5Kk5tScaWwGhifUNgD3ZeYK4L6yDnAusKK81gO3QTsAgKuBjwNnAVd3hMBtpe34fsM9+pAkNaRnaGTmd4CDE8qrgS1leQtwQUf9jmx7CDgxIpYAnwF2ZebBzHwZ2AUMl20nZOaDmZnAHROO1a0PSVJDpntPYyAz9wOUr6eU+lLg+Y52o6U2VX20S32qPiRJDVkww8eLLrWcRv3IOo1YT/sSFwMDA7RarSM9BAADC+GqMw5Pa9+jMd3xzoSxsbFG+2+Cc54fmppzE79DoH/znW5o/CgilmTm/nKJ6UCpjwKndrRbBrxQ6kMT6q1SX9al/VR9vElmbgQ2AgwODubQ0NBkTad0y53buHHPTOdob/suGep7n+NarRbT/X7NVc55fmhqzpdtuKfvfQJsHl7Ul/lO9/LUdmD8Cai1wLaO+qXlKapVwKFyaWkncE5EnFRugJ8D7CzbXo2IVeWpqUsnHKtbH5KkhvT8szoivk77LGFxRIzSfgrqemBrRKwDngMuKs13AOcBI8BrwOUAmXkwIq4FHintrsnM8ZvrV9B+QmshcG95MUUfkqSG9AyNzLx4kk1nd2mbwJWTHGcTsKlLfTdwepf6S936kCQ1x3eES5KqGRqSpGqGhiSpmqEhSapmaEiSqhkakqRqhoYkqZqhIUmqZmhIkqoZGpKkaoaGJKmaoSFJqmZoSJKqGRqSpGqGhiSpmqEhSapmaEiSqhkakqRqhoYkqZqhIUmqZmhIkqoZGpKkaoaGJKmaoSFJqmZoSJKqGRqSpGqGhiSpmqEhSapmaEiSqhkakqRqhoYkqZqhIUmqdsyHRkQMR8QzETESERuaHo8kzWfHdGhExHHArcC5wErg4ohY2eyoJGn+OqZDAzgLGMnMZzPzp8BdwOqGxyRJ89axHhpLgec71kdLTZLUgAVND6CH6FLLNzWKWA+sL6tjEfHMNPtbDLw4zX2nLW7od4+/oJE5N8w5zw/zas6fvOGo5/srNY2O9dAYBU7tWF8GvDCxUWZuBDYebWcRsTszB4/2OHOJc54fnPNbX7/me6xfnnoEWBERp0XE8cAaYHvDY5KkeeuYPtPIzMMR8XlgJ3AcsCkzn2x4WJI0bx3ToQGQmTuAHX3q7qgvcc1Bznl+cM5vfX2Zb2S+6b6yJEldHev3NCRJx5B5GRq9PpokIt4REXeX7Q9HxPL+j3JmVcz5ixHxVEQ8HhH3RUTV43fHstqPoImICyMiI2JOP2lTM9+I+K3y7/xkRHyt32OcaRU/178cEQ9ExGPlZ/u8JsY5kyJiU0QciIgnJtkeEXFz+Z48HhFnzugAMnNevWjfUP8B8KvA8cD3gZUT2vwu8DdleQ1wd9Pj7sOcPwn8Ulm+Yj7MubR7N/Ad4CFgsOlxz/K/8QrgMeCksn5K0+Puw5w3AleU5ZXAvqbHPQPz/nXgTOCJSbafB9xL+31uq4CHZ7L/+XimUfPRJKuBLWX5G8DZEdHtjYZzRc85Z+YDmflaWX2I9nti5rLaj6C5FvgL4Cf9HNwsqJnv7wC3ZubLAJl5oM9jnGk1c07ghLL8Hrq8z2uuyczvAAenaLIauCPbHgJOjIglM9X/fAyNmo8meaNNZh4GDgEn92V0s+NIP45lHe2/VOaynnOOiI8Cp2bmt/o5sFlS82/8AeADEfHvEfFQRAz3bXSzo2bOfwp8LiJGaT+F+Xv9GVqjZvXjl475R25nQc1Hk1R9fMkcUj2fiPgcMAj8xqyOaPZNOeeIeBtwE3BZvwY0y2r+jRfQvkQ1RPtM8l8j4vTMfGWWxzZbauZ8MbA5M2+MiF8Dvlrm/PPZH15jZvX313w806j5aJI32kTEAtqntVOdDh7rqj6OJSI+DfwJcH5mvt6nsc2WXnN+N3A60IqIfbSv/W6fwzfDa3+ut2Xm/2XmD4FnaIfIXFUz53XAVoDMfBB4J+3PpHorq/r/Pl3zMTRqPppkO7C2LF8I3J/lDtMc1XPO5VLN39IOjLl+rRt6zDkzD2Xm4sxcnpnLad/HOT8zdzcz3KNW83P9z7QfeCAiFtO+XPVsX0c5s2rm/BxwNkBEfJB2aPxPX0fZf9uBS8tTVKuAQ5m5f6YOPu8uT+UkH00SEdcAuzNzO3A77dPYEdpnGGuaG/HRq5zzXwLvAv6h3PN/LjPPb2zQR6lyzm8ZlfPdCZwTEU8BPwP+MDNfam7UR6dyzlcBfxcRf0D7Es1lc/wPQCLi67QvMS4u92quBt4OkJl/Q/vezXnACPAacPmM9j/Hv3+SpD6aj5enJEnTZGhIkqoZGpKkaoaGJKmaoSFJqmZoSJKqGRqSpGqGhiSp2v8DVRwG0I2q8yYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(\"C:/Users/kevin/Desktop/Career Prep/Kevin work Portfolio/Personal Project/sarcasm/train-balanced-sarcasm.csv\")\n",
    "print(train.shape)\n",
    "print(train.columns)\n",
    "\n",
    "#drop rows that have missing comments\n",
    "train.dropna(subset=['comment'], inplace=True)\n",
    "\n",
    "# Parse UNIX epoch timestamp as datetime: \n",
    "train.created_utc = pd.to_datetime(train.created_utc,infer_datetime_format=True) # Applies to original data , which had UNIX Epoch timestamp! \n",
    "\n",
    "train.describe()\n",
    "\n",
    "########\n",
    "train['label'].hist() #50 50 split\n",
    "\n",
    "##see a sample of comments\n",
    "train['comment'].sample(10)\n",
    "train[train.label == 1][\"comment\"].sample(10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Canada_girl</th>\n",
       "      <td>300</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chaoslab</th>\n",
       "      <td>252</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NeonDisease</th>\n",
       "      <td>422</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShyBiDude89</th>\n",
       "      <td>404</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ivsciguy</th>\n",
       "      <td>342</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mad-n-fla</th>\n",
       "      <td>318</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mindlessrabble</th>\n",
       "      <td>302</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pokemon_fetish</th>\n",
       "      <td>432</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Biffingston</th>\n",
       "      <td>845</td>\n",
       "      <td>0.499408</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleaze_bag_alert</th>\n",
       "      <td>251</td>\n",
       "      <td>0.498008</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  size      mean  sum\n",
       "author                               \n",
       "Canada_girl        300  0.500000  150\n",
       "Chaoslab           252  0.500000  126\n",
       "NeonDisease        422  0.500000  211\n",
       "ShyBiDude89        404  0.500000  202\n",
       "ivsciguy           342  0.500000  171\n",
       "mad-n-fla          318  0.500000  159\n",
       "mindlessrabble     302  0.500000  151\n",
       "pokemon_fetish     432  0.500000  216\n",
       "Biffingston        845  0.499408  422\n",
       "sleaze_bag_alert   251  0.498008  125"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many comments are in each subreddit?\n",
    "train.groupby([\"subreddit\"]).count()[\"comment\"].sort_values()\n",
    "\n",
    "#learn more about the subreddits and the frequency of sarcastic labels\n",
    "sub_df = train.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\n",
    "sub_df.sort_values(by='sum', ascending=False).head(10)\n",
    "sub_df[sub_df['size'] > 1000].sort_values(by='mean', ascending=False).head(10)\n",
    "\n",
    "#learn more about authors and the frequency of sarcastic labels\n",
    "author_df = train.groupby('author')['label'].agg([np.size, np.mean, np.sum])\n",
    "author_df.sort_values(by='sum', ascending=False).head(10)\n",
    "author_df[author_df['size'] > 250].sort_values(by='mean', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(758079,) (252694,) (758079,) (252694,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#take small sample for testing\\ntrain_texts_small = train_texts.sample(600, random_state=27)\\ny_train_small = y_train.sample(600, random_state=27)\\nvalid_texts_small = valid_texts.sample(600, random_state=27)\\ny_valid_small = y_valid.sample(600, random_state=27)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the df into training and validation parts\n",
    "train_texts, valid_texts, y_train, y_valid = \\\n",
    "        train_test_split(train['comment'], train['label'], random_state=17)\n",
    "        \n",
    "print(train_texts.shape, valid_texts.shape, y_train.shape, y_valid.shape)\n",
    "\n",
    "'''\n",
    "#take small sample for testing\n",
    "train_texts_small = train_texts.sample(600, random_state=27)\n",
    "y_train_small = y_train.sample(600, random_state=27)\n",
    "valid_texts_small = valid_texts.sample(600, random_state=27)\n",
    "y_valid_small = y_valid.sample(600, random_state=27)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Preprocessing with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter Stemming was inspired by the following source: \n",
    "https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0\n",
    "\n",
    "The classifier function's source is the following: \n",
    "https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Consider porter stemming\n",
    "import nltk, re\n",
    "def Tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    #words = re.search(r'\\w{1,}',str_input).lower().split()\n",
    "    porter_stemmer=nltk.PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, label_valid,is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, label_valid)\n",
    "    return metrics.classification_report(predictions, label_valid)\n",
    "\n",
    "\n",
    "#count vectors\n",
    "'''\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 3), max_features=50000, min_df=2, lowercase=True, max_df=0.9)\n",
    "count_vect.fit(train['comment'])\n",
    "\n",
    "#count vectors\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_texts)\n",
    "xvalid_count =  count_vect.transform(valid_texts)\n",
    "'''\n",
    "\n",
    "# word and n-gram level tf-idf\n",
    "#tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=50000, ngram_range=(1,3), min_df=2, lowercase=True, max_df=0.9)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', tokenizer=Tokenizer, max_features=50000, ngram_range=(1,2), min_df=2, lowercase=True, max_df=0.95)\n",
    "tfidf_vect.fit(train['comment'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_texts)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.6766800952931213\n",
      "Wall time: 10min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#try XGBoost on word- and ngram-level vectors\n",
    "#accuracy = train_model(xgboost.XGBClassifier(), xtrainSVD, y_train, xvalidSVD)\n",
    "accuracy = train_model(xgboost.XGBClassifier(n_estimators=400), xtrain_tfidf.tocsc(), y_train, xvalid_tfidf.tocsc(), y_valid)\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "#try XGBoost on count vectors\n",
    "#accuracy = train_model(xgboost.XGBClassifier(n_estimators=400), xtrain_count.tocsc(), y_train, xvalid_count.tocsc())\n",
    "#print(\"Xgb, Count Vectors: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.7227080975408993\n",
      "Wall time: 47.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#logistic regression\n",
    "accuracy = train_model(LogisticRegression(solver='lbfgs', random_state=17, max_iter=1000), xtrain_tfidf, y_train, xvalid_tfidf, y_valid)\n",
    "print(\"Logistic Regression: \", accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
